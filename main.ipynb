{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:16.939855Z",
     "start_time": "2025-09-28T18:08:16.935639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "import modules.data_setup as data_setup\n",
    "from modules.engine import train_step,test_step,train\n",
    "import os\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ],
   "id": "6fd7d9eb3fb917f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:16.951479Z",
     "start_time": "2025-09-28T18:08:16.949246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "version = torch.__version__\n",
    "print(f\"PyTorch Version: {version}\")"
   ],
   "id": "53f59231b8f5a57b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:16.961085Z",
     "start_time": "2025-09-28T18:08:16.957468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "data_path = Path('data')\n",
    "image_path = data_path / 'pizza_steak_sushi_20_percent'\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f'{image_path} exists')\n",
    "else:\n",
    "    print(f'{image_path} does not exist, creating...')\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(data_path / 'pizza_steak_sushi_20_percent.zip','wb') as f:\n",
    "         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\")\n",
    "         print('Downloading pizza_steak_sushi_20_percent data')\n",
    "         f.write(request.content)\n",
    "    with zipfile.ZipFile(data_path / 'pizza_steak_sushi_20_percent.zip','r') as zip_ref:\n",
    "        print(\"Extracting pizza_steak_sushi_20_percent.zip\")\n",
    "        zip_ref.extractall(image_path)\n",
    "\n",
    "    os.remove(data_path / 'pizza_steak_sushi_20_percent.zip')"
   ],
   "id": "d4c825d3736e97d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pizza_steak_sushi_20_percent exists\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:16.965620Z",
     "start_time": "2025-09-28T18:08:16.964047Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #device agnostic setting",
   "id": "261e0447e63abe0f",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:16.971839Z",
     "start_time": "2025-09-28T18:08:16.970351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed:int=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ],
   "id": "ce3e0429839a59a5",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:16.978939Z",
     "start_time": "2025-09-28T18:08:16.976460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir = image_path/'train'\n",
    "test_dir = image_path/'test'\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485,0.456,0.406],\n",
    "    std=[0.229,0.224,0.225]\n",
    ")\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "print(simple_transform)\n"
   ],
   "id": "edbf9f109660a420",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:16.987596Z",
     "start_time": "2025-09-28T18:08:16.985841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_augmentation_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "print(data_augmentation_transform)"
   ],
   "id": "75cc735b8af7ab6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:16.998046Z",
     "start_time": "2025-09-28T18:08:16.992745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader,test_dataloader,class_names = data_setup.create_dataloaders(\n",
    "                                                train_dir=train_dir,\n",
    "                                                test_dir=test_dir,\n",
    "                                                train_transform=simple_transform,\n",
    "                                                test_transform=simple_transform,\n",
    "                                                batch_size=32)\n",
    "train_dataloader,test_dataloader,class_names"
   ],
   "id": "41a2103bf4b7699a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x16b9c5430>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x16b90d910>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:17.006229Z",
     "start_time": "2025-09-28T18:08:17.002789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader_with_aug,test_dataloader,class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    train_transform=data_augmentation_transform,\n",
    "    test_transform=simple_transform,\n",
    "    batch_size=32\n",
    ")\n",
    "train_dataloader_with_aug,test_dataloader,class_names"
   ],
   "id": "7cc8646b65012610",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x16b90d160>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x16b9c4e90>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:17.012845Z",
     "start_time": "2025-09-28T18:08:17.011024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_shape = len(class_names)\n",
    "def create_effnetb2():\n",
    "\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seed()\n",
    "\n",
    "    model_classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features=1408,out_features=output_shape)\n",
    "    )\n",
    "\n",
    "    model.name = 'effnetb2'\n",
    "    print(f'[INFO] efficientnet-b2 model created: {model.name}')\n",
    "    return model"
   ],
   "id": "b2e2047eb1c85308",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:17.124168Z",
     "start_time": "2025-09-28T18:08:17.016098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = [5,10]\n",
    "effnetb2 = create_effnetb2()"
   ],
   "id": "703e79916bcc854f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] efficientnet-b2 model created: effnetb2\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:17.130950Z",
     "start_time": "2025-09-28T18:08:17.129425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = ['effnetb2']\n",
    "train_dataloaders = {\n",
    "    'data_without_augmentation': train_dataloader,\n",
    "    'data_with_augmentation': train_dataloader_with_aug,\n",
    "}"
   ],
   "id": "6bfa3c35f0b84850",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T18:08:36.643082Z",
     "start_time": "2025-09-28T18:08:17.135438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modules.save_model import save_model\n",
    "from modules.engine import train\n",
    "from modules.create_writer import create_writer\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "set_seed(42)\n",
    "\n",
    "experiment_number = 0\n",
    "\n",
    "for dataloader_name,train_dataloader in train_dataloaders.items():\n",
    "    for epochs in num_epochs:\n",
    "        for model_name in models:\n",
    "            experiment_number += 1\n",
    "            print(f'[INFO] Experiment number: {experiment_number}')\n",
    "            print(f'[INFO] Model name: {model_name}')\n",
    "            print(f'[INFO] Dataloader:{dataloader_name}')\n",
    "            print(f'[INFO] Epoch number: {epochs}')\n",
    "\n",
    "            if model_name == 'effnetb2':\n",
    "                model = create_effnetb2()\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "            train(\n",
    "                model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                epochs=epochs,\n",
    "                device=device,\n",
    "                writer=create_writer(\n",
    "                    experiment_name=dataloader_name,\n",
    "                    model_name=model_name,\n",
    "                    extra=f\"{epochs}_epochs\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            save_filepath = f'{model_name}_{dataloader_name}_{epochs}_epochs.pth'\n",
    "            save_model(\n",
    "                model=model,\n",
    "                target_dir=\"models\",\n",
    "                model_name=save_filepath,\n",
    "            )\n",
    "            print(\"-\"*50 + \"\\n\")"
   ],
   "id": "a7a492be9939dcc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Experiment number: 1\n",
      "[INFO] Model name: effnetb2\n",
      "[INFO] Dataloader:data_without_augmentation\n",
      "[INFO] Epoch number: 5\n",
      "[INFO] efficientnet-b2 model created: effnetb2\n",
      "[INFO] Created SummaryWriter. saving to runs/2025-09-28, 14-08-17/data_without_augmentation/effnetb2/5_epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[52]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     21\u001B[39m loss_fn = nn.CrossEntropyLoss()\n\u001B[32m     22\u001B[39m optimizer = torch.optim.Adam(model.parameters(),lr=\u001B[32m0.001\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcreate_writer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexperiment_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataloader_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mepochs\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m_epochs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     36\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m save_filepath = \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataloader_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_epochs.pth\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m     40\u001B[39m save_model(\n\u001B[32m     41\u001B[39m     model=model,\n\u001B[32m     42\u001B[39m     target_dir=\u001B[33m\"\u001B[39m\u001B[33mmodels\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     43\u001B[39m     model_name=save_filepath,\n\u001B[32m     44\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Simple-Food-Recognition/modules/engine.py:73\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, writer)\u001B[39m\n\u001B[32m     66\u001B[39m results = {\n\u001B[32m     67\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtrain_loss\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m     68\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtrain_acc\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m     69\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtest_loss\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m     70\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtest_acc\u001B[39m\u001B[33m\"\u001B[39m: [],\n\u001B[32m     71\u001B[39m }\n\u001B[32m     72\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(epochs)):\n\u001B[32m---> \u001B[39m\u001B[32m73\u001B[39m     train_loss, train_acc = \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     75\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     76\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     77\u001B[39m \u001B[43m        \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     78\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     79\u001B[39m     test_loss, test_acc = test_step(\n\u001B[32m     80\u001B[39m         model,\n\u001B[32m     81\u001B[39m         test_dataloader,\n\u001B[32m     82\u001B[39m         loss_fn,\n\u001B[32m     83\u001B[39m         device)\n\u001B[32m     84\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m|\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     85\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m|\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     86\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m Train Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m|\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     87\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m Test Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m|\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     88\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m Test Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     89\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Simple-Food-Recognition/modules/engine.py:24\u001B[39m, in \u001B[36mtrain_step\u001B[39m\u001B[34m(model, dataloader, optimizer, loss_fn, device)\u001B[39m\n\u001B[32m     22\u001B[39m train_loss += loss.item()\n\u001B[32m     23\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m optimizer.step()\n\u001B[32m     27\u001B[39m y_pred_class = torch.argmax(torch.softmax(y_pred, dim=\u001B[32m1\u001B[39m), dim=\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:647\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    637\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    638\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    639\u001B[39m         Tensor.backward,\n\u001B[32m    640\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    645\u001B[39m         inputs=inputs,\n\u001B[32m    646\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m647\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    648\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    827\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m829\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    830\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    831\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    832\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    833\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mRuntimeError\u001B[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
